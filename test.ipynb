{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of ground_truth: <class 'list'>\n",
      "Number of entries: 250\n",
      "Processed Ground Truth:\n",
      "File ID: 0, Keywords: [{'keyword': 'رقیق', 'start_time': 1.6009375, 'end_time': 2.6009374999999997}, {'keyword': 'خوششون', 'start_time': 5.1909374999999995, 'end_time': 6.1909374999999995}]\n",
      "File ID: 1, Keywords: [{'keyword': 'އޮފިސަރ', 'start_time': 2.65, 'end_time': 3.65}]\n",
      "Loaded test segments embeddings from test_segments_embeddings.pkl\n",
      "Processing File ID: 1, Expected Keywords: ['އޮފިސަރ']\n",
      "Loaded segments for 1 from test_segments_embeddings.pkl\n",
      "Updated test segments embeddings saved to test_segments_embeddings.pkl\n",
      "Processing File ID: 0, Expected Keywords: ['رقیق', 'خوششون']\n",
      "Loaded segments for 0 from test_segments_embeddings.pkl\n",
      "Updated test segments embeddings saved to test_segments_embeddings.pkl\n",
      "Keyword detection complete. Results saved to: test_keyword_detection_results.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "from scipy.spatial.distance import cdist\n",
    "import pickle\n",
    "\n",
    "# Define directories and files\n",
    "TRAIN_DIR = \"F:\\\\KWS\\\\TRAIN\\\\TRAIN\"\n",
    "TEST_AUDIO_DIR = \"F:\\\\KWS\\\\TEST_DUMMY_FINAL\\\\TEST_DUMMY_FINAL\"\n",
    "OUTPUT_FILE = \"test_keyword_detection_results.json\"\n",
    "THRESHOLD = 0.8  # More stringent threshold for keyword detection\n",
    "SAMPLING_RATE = 16000\n",
    "WINDOW_SIZE = 1.0   # in seconds for test audio segmentation\n",
    "STEP_SIZE = 0.5      # in seconds for test audio segmentation\n",
    "\n",
    "# Define paths to save embeddings\n",
    "CLASS_EMBEDDINGS_FILE = \"class_embeddings.pkl\"\n",
    "PROTOTYPES_FILE = \"prototypes.pkl\"\n",
    "TEST_SEGMENTS_EMBEDDINGS_FILE = \"test_segments_embeddings.pkl\"\n",
    "\n",
    "# Initialize Wav2Vec2 Processor and Model\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model.eval()\n",
    "\n",
    "# Device configuration (optional: use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def extract_embedding(audio_path, sr=SAMPLING_RATE):\n",
    "    audio, _ = librosa.load(audio_path, sr=sr)\n",
    "    inputs = processor(audio, sampling_rate=sr, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs.input_values)\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "    return embedding\n",
    "\n",
    "def segment_audio(audio_path, window_size=WINDOW_SIZE, step_size=STEP_SIZE, sr=SAMPLING_RATE):\n",
    "    audio, _ = librosa.load(audio_path, sr=sr)\n",
    "    hop_length = int(step_size * sr)\n",
    "    win_length = int(window_size * sr)\n",
    "\n",
    "    segments = []\n",
    "    start = 0\n",
    "    end = win_length\n",
    "    while end <= len(audio):\n",
    "        segment_audio = audio[start:end]\n",
    "        inputs = processor(segment_audio, sampling_rate=sr, return_tensors=\"pt\", padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs.input_values)\n",
    "            seg_emb = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "        segment_start_time = start / sr\n",
    "        segment_end_time = end / sr\n",
    "        segments.append((segment_start_time, segment_end_time, seg_emb))\n",
    "\n",
    "        start += hop_length\n",
    "        end = start + win_length\n",
    "\n",
    "    return segments\n",
    "\n",
    "def detect_keyword_in_segment(segment_emb, prototypes, threshold=THRESHOLD):\n",
    "    if not prototypes:\n",
    "        return None\n",
    "    proto_classes = list(prototypes.keys())\n",
    "    proto_mat = np.stack(list(prototypes.values()), axis=0)\n",
    "    distances = cdist([segment_emb], proto_mat, metric='cosine')[0]\n",
    "    min_dist_idx = np.argmin(distances)\n",
    "    min_dist = distances[min_dist_idx]\n",
    "    if min_dist < threshold:\n",
    "        return proto_classes[min_dist_idx]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Load Ground Truth\n",
    "GT_FILE = \"F:\\\\KWS\\\\TEST_DUMMY_FINAL\\\\TEST_DUMMY_FINAL\\\\GT_dummy_final.pkl\"\n",
    "\n",
    "with open(GT_FILE, 'rb') as f:\n",
    "    ground_truth_list = pickle.load(f)  # List of dictionaries\n",
    "\n",
    "# Verify Ground Truth Type\n",
    "print(\"Type of ground_truth:\", type(ground_truth_list))\n",
    "print(\"Number of entries:\", len(ground_truth_list))\n",
    "\n",
    "# Process Ground Truth to create a mapping: {file_id_no_ext: [keyword_str, ...]}\n",
    "ground_truth = {}  # Initialize empty dictionary\n",
    "\n",
    "for idx, entry in enumerate(ground_truth_list, 1):\n",
    "    for file_path, detections in entry.items():\n",
    "        # Extract file ID without extension\n",
    "        file_id_no_ext = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        if file_id_no_ext not in ground_truth:\n",
    "            ground_truth[file_id_no_ext] = []\n",
    "        ground_truth[file_id_no_ext].extend(detections)\n",
    "\n",
    "# Debug: Inspect the processed ground_truth\n",
    "print(\"Processed Ground Truth:\")\n",
    "for key, value in list(ground_truth.items())[:2]:  # Print first two entries\n",
    "    print(f\"File ID: {key}, Keywords: {value}\")\n",
    "\n",
    "# Only these test audio files\n",
    "test_audio_files = [\n",
    "    os.path.join(TEST_AUDIO_DIR, \"1.wav\"),\n",
    "    os.path.join(TEST_AUDIO_DIR, \"0.wav\")\n",
    "]\n",
    "\n",
    "# Initialize or load test segments embeddings\n",
    "if os.path.exists(TEST_SEGMENTS_EMBEDDINGS_FILE):\n",
    "    with open(TEST_SEGMENTS_EMBEDDINGS_FILE, 'rb') as f:\n",
    "        test_segments_embeddings = pickle.load(f)\n",
    "        print(f\"Loaded test segments embeddings from {TEST_SEGMENTS_EMBEDDINGS_FILE}\")\n",
    "else:\n",
    "    test_segments_embeddings = {}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for test_file in test_audio_files:\n",
    "    file_id = os.path.basename(test_file)\n",
    "    file_id_no_ext = os.path.splitext(file_id)[0]\n",
    "\n",
    "    # Retrieve expected keywords from ground_truth\n",
    "    expected_keywords = [detection[\"keyword\"] for detection in ground_truth.get(file_id_no_ext, [])]\n",
    "    \n",
    "    # Debug: Print expected keywords\n",
    "    print(f\"Processing File ID: {file_id_no_ext}, Expected Keywords: {expected_keywords}\")\n",
    "\n",
    "    # Filter prototypes to only those keywords expected in ground truth for this file\n",
    "    filtered_prototypes = {k: v for k, v in prototypes.items() if k in expected_keywords}\n",
    "    \n",
    "    # Check if embeddings for this test file are already saved\n",
    "    if file_id_no_ext in test_segments_embeddings:\n",
    "        segments = test_segments_embeddings[file_id_no_ext]\n",
    "        print(f\"Loaded segments for {file_id_no_ext} from {TEST_SEGMENTS_EMBEDDINGS_FILE}\")\n",
    "    else:\n",
    "        segments = segment_audio(test_file)\n",
    "        test_segments_embeddings[file_id_no_ext] = segments\n",
    "        print(f\"Computed and saved segments for {file_id_no_ext}\")\n",
    "    \n",
    "    # Optionally, save after processing each file to handle interruptions\n",
    "    with open(TEST_SEGMENTS_EMBEDDINGS_FILE, 'wb') as f:\n",
    "        pickle.dump(test_segments_embeddings, f)\n",
    "        print(f\"Updated test segments embeddings saved to {TEST_SEGMENTS_EMBEDDINGS_FILE}\")\n",
    "    \n",
    "    file_results = []\n",
    "\n",
    "    last_detected_keyword = None\n",
    "    last_detected_time = -999\n",
    "\n",
    "    for (start_t, end_t, seg_emb) in segments:\n",
    "        kw = detect_keyword_in_segment(seg_emb, filtered_prototypes, threshold=THRESHOLD)\n",
    "        if kw is not None:\n",
    "            # Simple post-processing to avoid multiple detections of the same keyword in overlapping windows\n",
    "            if kw != last_detected_keyword or (start_t - last_detected_time) > (STEP_SIZE * 2):\n",
    "                detection_entry = {\n",
    "                    \"keyword\": kw,\n",
    "                    \"start_time\": round(start_t, 2),\n",
    "                    \"end_time\": round(end_t, 2)\n",
    "                }\n",
    "                file_results.append(detection_entry)\n",
    "                last_detected_keyword = kw\n",
    "                last_detected_time = start_t\n",
    "\n",
    "    results[file_id_no_ext] = file_results\n",
    "\n",
    "# Save Results\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Keyword detection complete. Results saved to:\", OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
