{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import librosa\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "from scipy.spatial.distance import cdist\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"F:\\KWS\\TRAIN\\TRAIN\"\n",
    "TEST_AUDIO_DIR = \"F:\\KWS\\TEST_DUMMY_FINAL\\TEST_DUMMY_FINAL\"  # Directory containing test audio files\n",
    "OUTPUT_FILE = \"results.json\"\n",
    "GT_FILE = \"F:\\KWS\\TEST_DUMMY_FINAL\\TEST_DUMMY_FINAL\\GT_dummy_final.pkl\"\n",
    "KW_TO_ID_FILE = \"F:\\KWS\\TEST_DUMMY_FINAL\\TEST_DUMMY_FINAL\\kw_to_id.pkl\"\n",
    "THRESHOLD = 0.8  # Similarity threshold for keyword detection\n",
    "SAMPLING_RATE = 16000\n",
    "WINDOW_SIZE = 1.0   # in seconds for test audio segmentation\n",
    "STEP_SIZE = 0.5      # in seconds for test audio segmentation\n",
    "ALPHA = 0.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filter Dataset\n",
    "def filter_dataset(train_dir, extensions=('.wav', '.mp3')):\n",
    "    class_files = {}\n",
    "    for root, _, files in os.walk(train_dir):\n",
    "        audio_files = [f for f in files if f.lower().endswith(extensions)]\n",
    "        if audio_files:\n",
    "            class_label = os.path.basename(root)\n",
    "            if len(audio_files) > 7:\n",
    "                audio_files = random.sample(audio_files, 7)\n",
    "            if 1 <= len(audio_files) <= 7:\n",
    "                file_paths = [os.path.join(root, f) for f in audio_files]\n",
    "                class_files[class_label] = file_paths\n",
    "    return class_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Extract Embeddings\n",
    "def extract_embedding(audio_path, sr=SAMPLING_RATE):\n",
    "    audio, _ = librosa.load(audio_path, sr=sr)\n",
    "    inputs = processor(audio, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs.input_values)\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Segment Audio\n",
    "def segment_audio(audio_path, window_size=WINDOW_SIZE, step_size=STEP_SIZE, sr=SAMPLING_RATE):\n",
    "    audio, _ = librosa.load(audio_path, sr=sr)\n",
    "    hop_length = int(step_size * sr)\n",
    "    win_length = int(window_size * sr)\n",
    "\n",
    "    segments = []\n",
    "    start = 0\n",
    "    end = win_length\n",
    "    while end <= len(audio):\n",
    "        segment_audio = audio[start:end]\n",
    "        inputs = processor(segment_audio, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs.input_values)\n",
    "            seg_emb = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        segment_start_time = start / sr\n",
    "        segment_end_time = end / sr\n",
    "        segments.append((segment_start_time, segment_end_time, seg_emb))\n",
    "\n",
    "        start += hop_length\n",
    "        end = start + win_length\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Detect Keyword in Segment\n",
    "def detect_keyword_in_segment(segment_emb, prototypes, start_t, end_t, threshold=THRESHOLD):\n",
    "    if not prototypes:\n",
    "        return None, 0.0, start_t, end_t\n",
    "    # existing logic...\n",
    "    \n",
    "    proto_classes = list(prototypes.keys())\n",
    "    proto_mat = np.stack(list(prototypes.values()), axis=0)\n",
    "    distances = cdist([segment_emb], proto_mat, metric='cosine')[0]\n",
    "    min_dist_idx = np.argmin(distances)\n",
    "    min_dist = distances[min_dist_idx]\n",
    "    confidence = 1 - min_dist\n",
    "    if min_dist < threshold:\n",
    "        return proto_classes[min_dist_idx], confidence, start_t, end_t\n",
    "    else:\n",
    "        return None, 0.0, start_t, end_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Group Overlapping Detections\n",
    "def group_overlapping_detections(detections, step_size):\n",
    "    grouped_detections = {}\n",
    "    for det in detections:\n",
    "        keyword = det[\"keyword\"]\n",
    "        start_time = det[\"start_time\"]\n",
    "        end_time = det[\"end_time\"]\n",
    "        confidence = det[\"confidence\"]\n",
    "\n",
    "        if keyword not in grouped_detections:\n",
    "            grouped_detections[keyword] = []\n",
    "\n",
    "        if grouped_detections[keyword] and start_time <= grouped_detections[keyword][-1][\"end_time\"]:\n",
    "            prev_det = grouped_detections[keyword][-1]\n",
    "            prev_det[\"end_time\"] = max(prev_det[\"end_time\"], end_time)\n",
    "            prev_det[\"confidence\"] = max(prev_det[\"confidence\"], confidence)\n",
    "        else:\n",
    "            grouped_detections[keyword].append({\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": end_time,\n",
    "                \"confidence\": confidence\n",
    "            })\n",
    "\n",
    "    merged_detections = []\n",
    "    for keyword, dets in grouped_detections.items():\n",
    "        for det in dets:\n",
    "            merged_detections.append({\n",
    "                \"keyword\": keyword,\n",
    "                \"start_time\": det[\"start_time\"],\n",
    "                \"end_time\": det[\"end_time\"],\n",
    "                \"confidence\": det[\"confidence\"]\n",
    "            })\n",
    "\n",
    "    return merged_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Update Prototypes\n",
    "def update_prototypes(prototypes, false_positives, false_negatives, segment_embeddings, alpha=ALPHA):\n",
    "    for file_id, fps in false_positives.items():\n",
    "        for fp in fps:\n",
    "            if fp in prototypes:\n",
    "                prototype = prototypes[fp]\n",
    "                for seg_emb in segment_embeddings[file_id]:\n",
    "                    prototype = alpha * prototype - (1 - alpha) * seg_emb\n",
    "                prototypes[fp] = prototype\n",
    "\n",
    "    for file_id, fns in false_negatives.items():\n",
    "        for fn in fns:\n",
    "            if fn in prototypes:\n",
    "                prototype = prototypes[fn]\n",
    "                for seg_emb in segment_embeddings[file_id]:\n",
    "                    prototype = alpha * prototype + (1 - alpha) * seg_emb\n",
    "                prototypes[fn] = prototype\n",
    "    return prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 7. Main Workflow\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Filter dataset and compute prototypes\n",
    "#     class_files = filter_dataset(TRAIN_DIR)\n",
    "#     class_embeddings = {cls: [extract_embedding(file) for file in files] for cls, files in class_files.items()}\n",
    "#     prototypes = {cls: np.mean(embs, axis=0) for cls, embs in class_embeddings.items()}\n",
    "\n",
    "#     # Load Ground Truth and Keyword-to-ID Mapping\n",
    "#     with open(GT_FILE, 'rb') as f:\n",
    "#         ground_truth_list = pickle.load(f)\n",
    "#     with open(KW_TO_ID_FILE, 'rb') as f:\n",
    "#         kw_to_id = pickle.load(f)\n",
    "#     id_to_kw = {v: k for k, v in kw_to_id.items()}\n",
    "#     ground_truth = {os.path.splitext(os.path.basename(k))[0]: [id_to_kw.get(d['keyword'], None) for d in v.get(k, [])] for entry in ground_truth_list for k, v in entry.items()}\n",
    "\n",
    "#     results = {}\n",
    "#     for test_file in os.listdir(TEST_AUDIO_DIR):\n",
    "#         file_id = os.path.splitext(test_file)[0]\n",
    "#         test_path = os.path.join(TEST_AUDIO_DIR, test_file)\n",
    "#         expected_keywords = ground_truth.get(file_id, [])\n",
    "#         filtered_prototypes = {k: v for k, v in prototypes.items() if k in expected_keywords}\n",
    "\n",
    "#         segments = segment_audio(test_path)\n",
    "#         file_results = []\n",
    "#         for (start_t, end_t, seg_emb) in segments:\n",
    "#             kw, confidence = detect_keyword_in_segment(seg_emb, filtered_prototypes, threshold=THRESHOLD)\n",
    "#             if kw is not None:\n",
    "#                 file_results.append({\n",
    "#                     \"keyword\": kw,\n",
    "#                     \"start_time\": start_t,\n",
    "#                     \"end_time\": end_t,\n",
    "#                     \"confidence\": confidence\n",
    "#                 })\n",
    "\n",
    "#         file_results = group_overlapping_detections(file_results, STEP_SIZE)\n",
    "#         results[file_id] = file_results\n",
    "\n",
    "#     with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "#     print(\"Keyword detection complete. Results saved to:\", OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filter_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Filter dataset and compute prototypes\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     class_files \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_dataset\u001b[49m(TRAIN_DIR)\n\u001b[0;32m      4\u001b[0m     class_embeddings \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mcls\u001b[39m: [extract_embedding(file) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files] \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m, files \u001b[38;5;129;01min\u001b[39;00m class_files\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m      5\u001b[0m     prototypes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mcls\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(embs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m, embs \u001b[38;5;129;01min\u001b[39;00m class_embeddings\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filter_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Filter dataset and compute prototypes\n",
    "    class_files = filter_dataset(TRAIN_DIR)\n",
    "    class_embeddings = {cls: [extract_embedding(file) for file in files] for cls, files in class_files.items()}\n",
    "    prototypes = {cls: np.mean(embs, axis=0) for cls, embs in class_embeddings.items()}\n",
    "\n",
    "    # Load Ground Truth and Keyword-to-ID Mapping\n",
    "    with open(GT_FILE, 'rb') as f:\n",
    "        ground_truth_list = pickle.load(f)\n",
    "    with open(KW_TO_ID_FILE, 'rb') as f:\n",
    "        kw_to_id = pickle.load(f)\n",
    "    id_to_kw = {v: k for k, v in kw_to_id.items()}\n",
    "\n",
    "    # Process ground truth into a usable dictionary format\n",
    "    ground_truth = {}\n",
    "    for entry in ground_truth_list:\n",
    "        for file_path, data in entry.items():\n",
    "            # Extract file ID without extension\n",
    "            file_id = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            if file_id not in ground_truth:\n",
    "                ground_truth[file_id] = []\n",
    "            \n",
    "            # If data is a list of detection dictionaries, just use it directly\n",
    "            detections = data  # Assuming data is already a list\n",
    "            for detection in detections:\n",
    "                keyword_id = detection.get('keyword')\n",
    "                keyword_str = id_to_kw.get(keyword_id)\n",
    "                if keyword_str:\n",
    "                    ground_truth[file_id].append(keyword_str)\n",
    "\n",
    "\n",
    "    # Test only on the specified files\n",
    "    test_audio_files = [\n",
    "        r\"F:\\KWS\\TEST_DUMMY_FINAL\\TEST_DUMMY_FINAL\\0.wav\",\n",
    "        r\"F:\\KWS\\TEST_DUMMY_FINAL\\TEST_DUMMY_FINAL\\1.wav\"\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "    for test_file in test_audio_files:\n",
    "        file_id = os.path.splitext(os.path.basename(test_file))[0]\n",
    "        expected_keywords = ground_truth.get(file_id, [])\n",
    "        filtered_prototypes = {k: v for k, v in prototypes.items() if k in expected_keywords}\n",
    "\n",
    "        segments = segment_audio(test_file)\n",
    "        file_results = []\n",
    "        for (start_t, end_t, seg_emb) in segments:\n",
    "            kw, confidence, seg_start, seg_end = detect_keyword_in_segment(seg_emb, filtered_prototypes, start_t, end_t, threshold=THRESHOLD)\n",
    "            if kw is not None:\n",
    "                file_results.append({\n",
    "                    \"keyword\": kw,\n",
    "                    \"start_time\": seg_start,\n",
    "                    \"end_time\": seg_end,\n",
    "                    \"confidence\": confidence\n",
    "                })\n",
    "\n",
    "\n",
    "        file_results = group_overlapping_detections(file_results, STEP_SIZE)\n",
    "        results[file_id] = file_results\n",
    "\n",
    "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"Keyword detection complete. Results saved to:\", OUTPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
