{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bc9946d-0581-4d27-a64d-9a7c5211a1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\nishit\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\nishit\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\nishit\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nishit\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\nishit\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\nishit\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nishit\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nishit\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nishit\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nishit\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "11ff64cb-2c2d-4de3-a70b-78cd34b3fbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NISHIT\\anaconda3\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: Train: 2453, Val: 273, Test: 682\n",
      "Dataset preparation completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram, Resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define constants\n",
    "SAMPLE_RATE = 16000  # Target sample rate for all audio files\n",
    "N_MELS = 128         # Number of Mel filterbanks for spectrograms\n",
    "DATA_DIR = \"en\"      # Path to the Common Voice English dataset\n",
    "TARGET_LENGTH = 500  # Fixed length for the time dimension\n",
    "\n",
    "def preprocess_audio(file_path, sample_rate=SAMPLE_RATE, target_length=TARGET_LENGTH):\n",
    "    \"\"\"\n",
    "    Load and preprocess audio by resampling, converting to mel-spectrogram, \n",
    "    and ensuring a single channel with fixed dimensions.\n",
    "    \"\"\"\n",
    "    waveform, orig_sample_rate = torchaudio.load(file_path)\n",
    "    # Convert to mono (single channel)\n",
    "    if waveform.size(0) > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # Resample to target sample rate if needed\n",
    "    if orig_sample_rate != sample_rate:\n",
    "        resampler = Resample(orig_freq=orig_sample_rate, new_freq=sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # Generate Mel-Spectrogram\n",
    "    mel_transform = MelSpectrogram(sample_rate=sample_rate, n_mels=N_MELS)\n",
    "    mel_spectrogram = mel_transform(waveform)  # Shape: [1, freq, time]\n",
    "\n",
    "    # Pad or truncate the spectrogram to the target length\n",
    "    if mel_spectrogram.size(2) < target_length:\n",
    "        padding = target_length - mel_spectrogram.size(2)\n",
    "        mel_spectrogram = F.pad(mel_spectrogram, (0, padding))  # Pad along the time dimension\n",
    "    else:\n",
    "        mel_spectrogram = mel_spectrogram[:, :, :target_length]  # Truncate if longer than target_length\n",
    "    \n",
    "    return mel_spectrogram\n",
    "\n",
    "\n",
    "def load_metadata(data_dir):\n",
    "    \"\"\"\n",
    "    Load metadata CSV containing audio file paths and corresponding transcriptions.\n",
    "    \"\"\"\n",
    "    metadata_path = os.path.join(data_dir, \"validated.tsv\")\n",
    "    metadata = pd.read_csv(metadata_path, sep=\"\\t\")\n",
    "    # Keep only necessary columns\n",
    "    metadata = metadata[[\"path\", \"sentence\"]]\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def prepare_dataset(data_dir, sample_rate=SAMPLE_RATE, test_size=0.2, val_size=0.1):\n",
    "    \"\"\"\n",
    "    Load the dataset, preprocess audio, and split into train/val/test sets.\n",
    "    \"\"\"\n",
    "    # Load metadata\n",
    "    metadata = load_metadata(data_dir)\n",
    "    # Prepend data directory to file paths\n",
    "    metadata[\"path\"] = metadata[\"path\"].apply(lambda x: os.path.join(data_dir, \"clips\", x))\n",
    "    \n",
    "    # Preprocess audio files and create feature-label pairs\n",
    "    features, labels = [], []\n",
    "    for _, row in metadata.iterrows():\n",
    "        try:\n",
    "            mel_spectrogram = preprocess_audio(row[\"path\"], sample_rate)\n",
    "            features.append(mel_spectrogram)\n",
    "            labels.append(row[\"sentence\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {row['path']}: {e}\")\n",
    "    \n",
    "    # Split into train, validation, and test sets\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "        features, labels, test_size=test_size, random_state=42\n",
    "    )\n",
    "    train_features, val_features, train_labels, val_labels = train_test_split(\n",
    "        train_features, train_labels, test_size=val_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset split: Train: {len(train_features)}, Val: {len(val_features)}, Test: {len(test_features)}\")\n",
    "    return (train_features, train_labels), (val_features, val_labels), (test_features, test_labels)\n",
    "\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    # Prepare dataset\n",
    "    print(\"Preparing dataset...\")\n",
    "    (train_data, train_labels), (val_data, val_labels), (test_data, test_labels) = prepare_dataset(DATA_DIR)\n",
    "    \n",
    "    # Save splits for later use\n",
    "    torch.save({\"features\": train_data, \"labels\": train_labels}, \"train_data.pt\")\n",
    "    torch.save({\"features\": val_data, \"labels\": val_labels}, \"val_data.pt\")\n",
    "    torch.save({\"features\": test_data, \"labels\": test_labels}, \"test_data.pt\")\n",
    "    \n",
    "    print(\"Dataset preparation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9755b82-3d62-44c2-98ef-61fadce51f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160027c7-4a2f-4ae7-b9b5-15bbce123f28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NISHIT\\AppData\\Local\\Temp\\ipykernel_23136\\2105293484.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_data = torch.load(\"train_data.pt\")\n",
      "C:\\Users\\NISHIT\\AppData\\Local\\Temp\\ipykernel_23136\\2105293484.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_data = torch.load(\"val_data.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Features Shape: torch.Size([2453, 1, 128, 500])\n",
      "Train Labels Shape: torch.Size([2453])\n",
      "Using device: cpu\n",
      "Unique labels: tensor([  23,   53,   76,   97,  242,  482,  489,  734,  751,  831, 1060, 1316,\n",
      "        1596, 1630, 1665, 1765, 2056, 2175, 2176, 2306, 2494])\n",
      "Updated num_classes: 2725\n",
      "Batch 1 labels: tensor([ 109,  295,  311,  428,  459,  524,  536,  748,  819,  822,  899, 1001,\n",
      "        1104, 1368, 1372, 1523, 1649, 1776, 1857, 1871, 1926, 1947, 1970, 1979,\n",
      "        2071, 2332, 2443, 2472, 2562, 2585, 2638, 2656])\n",
      "Batch 2 labels: tensor([  47,  148,  220,  423,  436,  505,  622,  629,  728,  814,  836, 1080,\n",
      "        1285, 1595, 1596, 1599, 1668, 1810, 1902, 1958, 2118, 2128, 2191, 2303,\n",
      "        2389, 2470, 2506, 2537, 2538, 2632, 2670, 2708])\n",
      "Batch 3 labels: tensor([ 380,  439,  474,  494,  725,  745,  990, 1013, 1128, 1178, 1279, 1315,\n",
      "        1366, 1405, 1410, 1451, 1527, 1608, 1650, 1701, 1786, 2041, 2188, 2301,\n",
      "        2306, 2329, 2415, 2430, 2445, 2454, 2478, 2576])\n",
      "Batch 4 labels: tensor([   7,   25,   77,   98,  194,  225,  495,  604,  719,  825,  915,  920,\n",
      "        1016, 1045, 1165, 1230, 1491, 1629, 1655, 1846, 1880, 1920, 1934, 2113,\n",
      "        2186, 2209, 2223, 2253, 2255, 2449, 2648, 2713])\n",
      "Batch 5 labels: tensor([  40,  105,  210,  381,  515,  600,  715,  729,  754,  790,  815,  906,\n",
      "         987, 1059, 1213, 1242, 1263, 1412, 1479, 1494, 1505, 1735, 2088, 2144,\n",
      "        2208, 2337, 2344, 2488, 2566, 2660, 2667, 2699])\n",
      "Batch 6 labels: tensor([ 132,  186,  197,  200,  417,  453,  565,  666,  677,  868, 1079, 1094,\n",
      "        1096, 1183, 1219, 1271, 1443, 1473, 1542, 1605, 1667, 1675, 1765, 1919,\n",
      "        2108, 2150, 2318, 2439, 2511, 2518, 2553, 2621])\n",
      "Batch 7 labels: tensor([ 172,  307,  457,  483,  500,  607,  673,  705,  775,  804,  878,  894,\n",
      "         942, 1004, 1060, 1254, 1389, 1394, 1400, 1461, 1480, 1531, 1561, 1612,\n",
      "        1763, 1874, 1956, 1987, 2044, 2198, 2636, 2701])\n",
      "Batch 8 labels: tensor([  62,  152,  161,  218,  313,  320,  329,  336,  461,  484,  576,  632,\n",
      "         648,  650,  697,  817,  926, 1068, 1236, 1357, 1407, 1464, 1490, 1847,\n",
      "        1849, 1899, 2136, 2138, 2264, 2288, 2414, 2688])\n",
      "Batch 9 labels: tensor([ 114,  121,  137,  315,  372,  598,  845,  912, 1049, 1136, 1198, 1201,\n",
      "        1305, 1367, 1434, 1449, 1604, 1626, 1737, 1802, 1832, 1853, 1961, 2027,\n",
      "        2269, 2314, 2319, 2358, 2437, 2693, 2700, 2717])\n",
      "Batch 10 labels: tensor([  26,   30,  104,  231,  277,  296,  440,  514,  710,  863, 1050, 1070,\n",
      "        1240, 1436, 1444, 1456, 1519, 1553, 1794, 1814, 1976, 2096, 2103, 2104,\n",
      "        2121, 2260, 2262, 2291, 2292, 2394, 2416, 2521])\n",
      "Batch 11 labels: tensor([  56,   68,   82,  164,  481,  595,  616,  799,  831,  834,  923,  955,\n",
      "        1325, 1336, 1442, 1452, 1484, 1664, 1681, 1723, 1768, 1942, 1989, 2056,\n",
      "        2157, 2171, 2232, 2294, 2350, 2384, 2668, 2698])\n",
      "Batch 12 labels: tensor([ 113,  261,  281,  287,  360,  408,  446,  492,  573,  630,  649,  701,\n",
      "         746,  823,  849,  904, 1009, 1119, 1274, 1409, 1437, 1481, 1558, 1744,\n",
      "        1779, 1784, 2060, 2190, 2371, 2545, 2568, 2680])\n",
      "Batch 13 labels: tensor([ 205,  317,  319,  637,  704,  896,  924,  950, 1217, 1258, 1306, 1395,\n",
      "        1422, 1470, 1503, 1586, 1658, 1677, 1699, 1848, 1876, 1884, 2033, 2084,\n",
      "        2131, 2216, 2240, 2345, 2453, 2465, 2542, 2619])\n",
      "Batch 14 labels: tensor([   2,   78,  217,  248,  309,  509,  546,  627,  631,  767, 1103, 1146,\n",
      "        1243, 1257, 1335, 1556, 1678, 1718, 1742, 1747, 1845, 1882, 2080, 2147,\n",
      "        2156, 2251, 2270, 2316, 2496, 2569, 2672, 2711])\n",
      "Batch 15 labels: tensor([  55,  136,  293,  341,  497,  570,  611,  762,  879,  887,  913,  959,\n",
      "        1069, 1291, 1401, 1472, 1540, 1543, 1562, 1579, 1764, 1785, 1823, 1911,\n",
      "        1957, 1994, 2055, 2228, 2266, 2432, 2541, 2552])\n",
      "Batch 16 labels: tensor([  54,  314,  364,  501,  520,  542,  558,  582,  583,  874,  928, 1019,\n",
      "        1040, 1359, 1428, 1447, 1483, 1591, 1636, 1694, 1811, 1869, 1936, 2278,\n",
      "        2383, 2420, 2450, 2474, 2483, 2493, 2593, 2618])\n",
      "Batch 17 labels: tensor([ 170,  300,  356,  445,  467,  526,  608,  654,  671,  713,  788,  956,\n",
      "         965, 1140, 1228, 1262, 1269, 1347, 1360, 1695, 1702, 1861, 2040, 2135,\n",
      "        2169, 2170, 2241, 2299, 2526, 2563, 2575, 2616])\n",
      "Batch 18 labels: tensor([  99,  151,  213,  470,  473,  528,  624,  759,  907,  916,  947,  960,\n",
      "        1251, 1342, 1349, 1418, 1424, 1471, 1541, 1684, 1721, 1820, 1837, 1858,\n",
      "        1983, 2021, 2043, 2219, 2265, 2403, 2674, 2702])\n",
      "Batch 19 labels: tensor([  75,   87,  180,  193,  227,  371,  829,  943, 1100, 1134, 1162, 1283,\n",
      "        1284, 1324, 1441, 1565, 1590, 1645, 1821, 1905, 1941, 2058, 2163, 2196,\n",
      "        2212, 2308, 2324, 2356, 2462, 2549, 2675, 2697])\n",
      "Batch 20 labels: tensor([ 123,  221,  233,  251,  303,  613,  691,  883,  900,  978, 1006, 1018,\n",
      "        1024, 1203, 1304, 1310, 1322, 1348, 1350, 1508, 1534, 1567, 1633, 1998,\n",
      "        2038, 2148, 2243, 2330, 2447, 2485, 2491, 2642])\n",
      "Batch 21 labels: tensor([  20,   35,   67,  106,  223,  253,  610,  824,  851,  852,  857,  935,\n",
      "        1026, 1054, 1055, 1076, 1149, 1154, 1384, 1566, 1603, 1660, 1691, 2078,\n",
      "        2372, 2374, 2497, 2519, 2560, 2571, 2581, 2595])\n",
      "Batch 22 labels: tensor([ 184,  476,  490,  541,  548,  574,  580,  727,  932,  969, 1209, 1267,\n",
      "        1289, 1358, 1550, 1573, 1630, 1734, 1825, 1827, 2051, 2079, 2097, 2165,\n",
      "        2311, 2398, 2422, 2516, 2520, 2534, 2546, 2657])\n",
      "Batch 23 labels: tensor([  72,  173,  250,  415,  672,  921,  995, 1008, 1081, 1085, 1091, 1121,\n",
      "        1430, 1445, 1564, 1670, 1696, 1715, 1788, 1789, 1799, 1852, 1906, 1909,\n",
      "        2143, 2146, 2238, 2279, 2284, 2353, 2610, 2647])\n",
      "Batch 24 labels: tensor([ 176,  203,  449,  480,  485,  519,  628,  699,  901,  933,  968, 1041,\n",
      "        1135, 1153, 1312, 1351, 1506, 1782, 1801, 1812, 1893, 1925, 2081, 2210,\n",
      "        2297, 2300, 2351, 2409, 2431, 2530, 2556, 2587])\n",
      "Batch 25 labels: tensor([  43,  249,  365,  452,  512,  714,  780,  843,  853,  871, 1025, 1056,\n",
      "        1077, 1170, 1208, 1321, 1392, 1455, 1500, 1635, 1792, 1804, 1948, 1950,\n",
      "        2217, 2325, 2387, 2468, 2509, 2606, 2620, 2678])\n",
      "Batch 26 labels: tensor([  19,   27,  141,  237,  238,  578,  605,  652,  732,  925,  930, 1106,\n",
      "        1276, 1334, 1374, 1485, 1521, 1643, 1680, 1835, 1866, 1875, 1991, 2140,\n",
      "        2202, 2244, 2281, 2307, 2397, 2582, 2598, 2709])\n",
      "Batch 27 labels: tensor([  66,  112,  255,  348,  591,  752,  813, 1084, 1102, 1255, 1423, 1453,\n",
      "        1458, 1654, 1659, 1672, 1687, 1816, 2020, 2026, 2031, 2036, 2091, 2105,\n",
      "        2173, 2175, 2195, 2261, 2328, 2489, 2533, 2658])\n",
      "Batch 28 labels: tensor([   5,  144,  278,  386,  404,  437,  464,  544,  599,  726,  777,  808,\n",
      "         870, 1036, 1058, 1138, 1200, 1340, 1606, 1736, 1781, 1917, 2159, 2185,\n",
      "        2189, 2192, 2370, 2378, 2386, 2551, 2558, 2683])\n",
      "Batch 29 labels: tensor([ 154,  242,  280,  466,  518,  557,  559,  586,  712,  724,  786,  886,\n",
      "         944, 1187, 1191, 1244, 1248, 1426, 1547, 1577, 1749, 2007, 2015, 2029,\n",
      "        2067, 2220, 2315, 2365, 2502, 2525, 2597, 2703])\n",
      "Batch 30 labels: tensor([ 211,  422,  482,  502,  532,  696,  702,  760,  802,  892,  988,  994,\n",
      "        1037, 1107, 1113, 1148, 1238, 1249, 1327, 1370, 1520, 1719, 1953, 1981,\n",
      "        1985, 2016, 2025, 2122, 2155, 2296, 2570, 2669])\n",
      "Batch 31 labels: tensor([  33,  100,  140,  160,  274,  385,  593,  838,  861,  982, 1032, 1034,\n",
      "        1241, 1261, 1275, 1307, 1362, 1373, 1399, 1497, 1529, 1572, 1743, 1751,\n",
      "        1867, 1913, 1999, 2052, 2095, 2174, 2230, 2527])\n",
      "Batch 32 labels: tensor([ 150,  361,  384,  441,  537,  636,  774,  908,  964,  997, 1234, 1272,\n",
      "        1319, 1338, 1339, 1365, 1376, 1388, 1460, 1576, 1582, 1634, 1870, 1946,\n",
      "        2086, 2222, 2245, 2283, 2498, 2589, 2590, 2722])\n",
      "Batch 33 labels: tensor([  42,   50,  139,  212,  254,  266,  369,  399,  468,  489,  540,  730,\n",
      "         744,  783,  888,  931, 1429, 1493, 1710, 1759, 1868, 1890, 1928, 2003,\n",
      "        2123, 2149, 2160, 2225, 2346, 2408, 2444, 2650])\n",
      "Batch 34 labels: tensor([  28,  118,  352,  362,  510,  669,  812,  821, 1132, 1172, 1222, 1361,\n",
      "        1403, 1499, 1509, 1533, 1539, 1657, 1753, 1755, 1904, 1907, 1990, 2229,\n",
      "        2287, 2310, 2390, 2395, 2475, 2500, 2557, 2682])\n",
      "Batch 35 labels: tensor([  52,   59,   86,  190,  291,  383,  391,  425,  430,  458,  503,  543,\n",
      "         561,  698,  700,  807,  826,  897,  914,  957, 1047, 1052, 1206, 1229,\n",
      "        1535, 1578, 1761, 1923, 2134, 2137, 2375, 2686])\n",
      "Batch 36 labels: tensor([  21,  174,  177,  306,  389,  731,  753,  867,  869,  881,  998, 1063,\n",
      "        1281, 1323, 1487, 1632, 1757, 1818, 1833, 1841, 2115, 2127, 2176, 2338,\n",
      "        2348, 2366, 2388, 2441, 2481, 2540, 2548, 2653])\n",
      "Batch 37 labels: tensor([ 234,  323,  344,  398,  493,  555,  562,  575,  621, 1111, 1122, 1239,\n",
      "        1256, 1316, 1375, 1397, 1415, 1417, 1462, 1773, 1937, 2032, 2047, 2099,\n",
      "        2109, 2114, 2231, 2295, 2421, 2532, 2662, 2691])\n",
      "Batch 38 labels: tensor([ 128,  153,  198,  201,  206,  301,  429,  462,  689,  801, 1075, 1097,\n",
      "        1112, 1211, 1214, 1329, 1369, 1379, 1488, 1557, 1570, 1607, 1705, 1725,\n",
      "        1730, 1775, 1831, 2017, 2298, 2331, 2411, 2463])\n",
      "Batch 39 labels: tensor([ 108,  111,  188,  208,  424,  426,  554,  840,  850,  927,  979, 1042,\n",
      "        1280, 1435, 1478, 1546, 1733, 1796, 1951, 1960, 2046, 2057, 2341, 2391,\n",
      "        2392, 2427, 2523, 2559, 2600, 2609, 2639, 2671])\n",
      "Batch 40 labels: tensor([ 335,  350,  370,  374,  414,  569,  625,  720, 1071, 1117, 1171, 1205,\n",
      "        1224, 1296, 1301, 1314, 1364, 1504, 1651, 1703, 1716, 1910, 1924, 1940,\n",
      "        2207, 2257, 2455, 2459, 2508, 2524, 2604, 2707])\n",
      "Batch 41 labels: tensor([   0,    4,  185,  258,  332,  355,  550,  614,  640,  670,  766,  830,\n",
      "         846,  866,  984,  999, 1051, 1062, 1092, 1156, 1273, 1353, 1396, 1665,\n",
      "        1754, 1803, 1819, 2005, 2201, 2402, 2555, 2651])\n",
      "Batch 42 labels: tensor([  34,  110,  264,  403,  460,  506,  533,  612,  986,  991, 1014, 1133,\n",
      "        1196, 1393, 1402, 1486, 1492, 1568, 1688, 1727, 1855, 2111, 2180, 2280,\n",
      "        2282, 2424, 2529, 2550, 2611, 2655, 2673, 2687])\n",
      "Batch 43 labels: tensor([  94,  195,  257,  270,  305,  340,  382,  617,  735,  832,  905,  909,\n",
      "         972, 1292, 1294, 1303, 1378, 1386, 1404, 1600, 1669, 1704, 1896, 1964,\n",
      "        2083, 2151, 2224, 2320, 2515, 2564, 2646, 2723])\n",
      "Batch 44 labels: tensor([  64,   81,   83,  393,  406,  442,  678,  756,  768,  848,  898, 1116,\n",
      "        1130, 1226, 1237, 1266, 1299, 1387, 1498, 1706, 1791, 1843, 1980, 1992,\n",
      "        2045, 2074, 2200, 2233, 2380, 2477, 2661, 2705])\n",
      "Batch 45 labels: tensor([  32,   38,   41,   44,  149,  230,  338,  516,  530,  584,  687,  743,\n",
      "         842,  980, 1173, 1181, 1463, 1525, 1616, 1639, 1815, 1851, 1914, 1916,\n",
      "        1997, 2124, 2161, 2236, 2289, 2293, 2327, 2695])\n",
      "Batch 46 labels: tensor([ 331,  377,  456,  567,  623,  633,  647,  763,  967, 1029, 1078, 1177,\n",
      "        1216, 1225, 1246, 1298, 1332, 1341, 1450, 1526, 1532, 1548, 1774, 1894,\n",
      "        2214, 2340, 2405, 2452, 2471, 2484, 2547, 2715])\n",
      "Batch 47 labels: tensor([  31,   76,   92,  107,  115,  171,  283,  286,  367,  401,  447,  508,\n",
      "         522,  634,  662,  755,  772,  953, 1022, 1072, 1098, 1220, 1265, 1346,\n",
      "        1552, 1690, 1798, 1813, 1856, 1969, 2063, 2183])\n",
      "Batch 48 labels: tensor([  97,  135,  345,  390,  433,  478,  486,  641,  690,  792,  810,  895,\n",
      "        1105, 1115, 1151, 1331, 1354, 1416, 1496, 1536, 1574, 1594, 1611, 1648,\n",
      "        1652, 1707, 2006, 2204, 2305, 2373, 2467, 2684])\n",
      "Batch 49 labels: tensor([  10,   73,  103,  318,  330,  347,  606,  667,  734,  872,  939, 1005,\n",
      "        1144, 1182, 1253, 1268, 1330, 1467, 1559, 1580, 1602, 1640, 1647, 1661,\n",
      "        1729, 1745, 2182, 2226, 2461, 2482, 2513, 2712])\n",
      "Batch 50 labels: tensor([  17,   29,   57,   74,   85,  146,  207,  268,  325,  343,  402,  477,\n",
      "         579,  796,  833,  873, 1048, 1087, 1295, 1320, 1469, 1569, 1623, 1834,\n",
      "        2059, 2153, 2194, 2322, 2343, 2379, 2486, 2622])\n",
      "Batch 51 labels: tensor([ 102,  130,  412,  448,  547,  668,  684,  736,  773,  946,  963, 1088,\n",
      "        1095, 1124, 1290, 1377, 1406, 1673, 1674, 1713, 1842, 1859, 1927, 1967,\n",
      "        2206, 2360, 2492, 2554, 2591, 2629, 2659, 2676])\n",
      "Batch 52 labels: tensor([ 222,  226,  400,  420,  421,  596,  722,  747,  793,  952,  962, 1015,\n",
      "        1020, 1053, 1064, 1083, 1259, 1297, 1302, 1425, 1489, 1510, 1524, 1756,\n",
      "        1878, 1959, 2053, 2211, 2419, 2460, 2494, 2724])\n",
      "Batch 53 labels: tensor([  53,   90,  126,  145,  241,  265,  273,  279,  496,  507,  594,  646,\n",
      "         659,  685,  816,  971,  974,  985, 1356, 1518, 1584, 1589, 1618, 1712,\n",
      "        1829, 1982, 2076, 2129, 2276, 2458, 2503, 2505])\n",
      "Batch 54 labels: tensor([ 142,  157,  244,  354,  521,  549,  658,  717,  739,  938, 1043, 1141,\n",
      "        1161, 1186, 1440, 1457, 1642, 1646, 1682, 1770, 1912, 2098, 2110, 2259,\n",
      "        2277, 2429, 2599, 2631, 2641, 2654, 2689, 2694])\n",
      "Batch 55 labels: tensor([  84,  101,  192,  214,  373,  395,  413,  539,  553,  556,  809,  890,\n",
      "         954,  970,  975, 1152, 1537, 1563, 1709, 1726, 1750, 1806, 2034, 2049,\n",
      "        2087, 2199, 2203, 2271, 2480, 2577, 2615, 2637])\n",
      "Batch 56 labels: tensor([  39,   45,   96,  131,  182,  349,  376,  572,  789,  811,  847, 1142,\n",
      "        1143, 1286, 1433, 1597, 1698, 1769, 1885, 1949, 1963, 1974, 1978, 1988,\n",
      "        2004, 2028, 2272, 2381, 2406, 2433, 2487, 2623])\n",
      "Batch 57 labels: tensor([ 322,  353,  535,  590,  676,  757,  771,  844,  876,  922,  948,  961,\n",
      "         976, 1033, 1065, 1313, 1588, 1593, 1601, 1615, 1888, 2120, 2139, 2152,\n",
      "        2321, 2354, 2410, 2490, 2522, 2565, 2607, 2706])\n",
      "Batch 58 labels: tensor([   9,   16,  165,  239,  245,  246,  363,  375,  396,  409,  471,  475,\n",
      "         619,  638,  966, 1035, 1163, 1218, 1277, 1311, 1371, 1382, 1385, 1513,\n",
      "        1777, 1898, 1993, 2132, 2177, 2362, 2536, 2579])\n",
      "Batch 59 labels: tensor([  46,  124,  183,  219,  262,  302,  419,  639,  820,  929, 1023, 1044,\n",
      "        1067, 1118, 1126, 1643, 1653, 1685, 1741, 1748, 1828, 1930, 1933, 1954,\n",
      "        2072, 2254, 2268, 2323, 2407, 2608, 2617, 2633])\n",
      "Batch 60 labels: tensor([ 288,  290,  304,  351,  388,  407,  551,  560,  642,  683,  709,  711,\n",
      "         885,  949,  989, 1150, 1223, 1287, 1288, 1476, 1516, 1628, 1795, 2010,\n",
      "        2070, 2235, 2309, 2355, 2495, 2630, 2652, 2720])\n",
      "Batch 61 labels: tensor([  61,   93,  133,  324,  410,  455,  563,  740,  764,  806,  818,  839,\n",
      "         882,  911,  934, 1184, 1585, 1762, 1824, 1839, 1840, 1895, 1918, 1972,\n",
      "        2069, 2141, 2145, 2172, 2187, 2250, 2290, 2580])\n",
      "Batch 62 labels: tensor([  14,  134,  147,  158,  163,  167,  191,  378,  387,  472,  525,  603,\n",
      "         626,  741,  776,  875,  903, 1131, 1210, 1683, 1740, 1809, 2030, 2050,\n",
      "        2166, 2215, 2417, 2479, 2544, 2602, 2626, 2721])\n",
      "Batch 63 labels: tensor([  18,   23,   48,   63,   79,  310,  346,  358,  359,  368,  587,  635,\n",
      "         781,  782,  880, 1123, 1337, 1522, 1545, 1622, 1624, 1641, 1772, 1886,\n",
      "        1889, 1975, 2000, 2018, 2125, 2399, 2401, 2440])\n",
      "Batch 64 labels: tensor([ 202,  284,  431,  597,  686,  749, 1010, 1066, 1158, 1169, 1185, 1195,\n",
      "        1197, 1270, 1390, 1398, 1438, 1475, 1554, 1862, 1891, 1897, 1900, 1932,\n",
      "        1971, 1977, 2093, 2312, 2333, 2426, 2588, 2624])\n",
      "Batch 65 labels: tensor([   1,   49,  294,  342,  432,  523,  538,  577,  615,  618,  919, 1057,\n",
      "        1074, 1089, 1147, 1179, 1465, 1482, 1502, 1551, 1739, 1850, 1860, 1943,\n",
      "        1996, 2022, 2073, 2218, 2425, 2451, 2578, 2664])\n",
      "Batch 66 labels: tensor([  22,  216,  285,  328,  334,  653,  769,  945, 1000, 1030, 1125, 1129,\n",
      "        1352, 1421, 1592, 1637, 1656, 1728, 1731, 1787, 1797, 1826, 1864, 1973,\n",
      "        2101, 2102, 2126, 2237, 2249, 2367, 2625, 2644])\n",
      "Batch 67 labels: tensor([ 119,  585,  588,  674,  758,  856,  859,  910,  996, 1028, 1061, 1082,\n",
      "        1250, 1355, 1419, 1511, 1515, 1614, 1700, 1717, 1800, 1838, 1931, 1966,\n",
      "        1995, 2014, 2092, 2094, 2205, 2412, 2507, 2679])\n",
      "Batch 68 labels: tensor([  11,  143,  155,  292,  394,  661,  761,  791,  798,  841,  865,  936,\n",
      "         983, 1011, 1090, 1160, 1166, 1232, 1233, 1414, 1439, 1512, 1679, 1887,\n",
      "        2035, 2423, 2438, 2514, 2572, 2583, 2584, 2685])\n",
      "Batch 69 labels: tensor([   8,   13,   58,  187,  243,  321,  454,  499,  787,  794,  937,  951,\n",
      "        1099, 1176, 1318, 1448, 1676, 1686, 1732, 1830, 1883, 1952, 2112, 2326,\n",
      "        2334, 2335, 2357, 2364, 2376, 2428, 2601, 2677])\n",
      "Batch 70 labels: tensor([ 169,  240,  469,  517,  680,  973,  993, 1017, 1215, 1235, 1300, 1345,\n",
      "        1495, 1836, 1881, 1903, 2042, 2064, 2077, 2106, 2133, 2273, 2275, 2361,\n",
      "        2456, 2466, 2535, 2543, 2574, 2634, 2645, 2714])\n",
      "Batch 71 labels: tensor([ 156,  166,  224,  272,  411,  434,  491,  589,  737,  751,  805,  837,\n",
      "         858, 1073, 1109, 1167, 1420, 1431, 1689, 1752, 1807, 1817, 1844, 1872,\n",
      "        1965, 2082, 2089, 2168, 2267, 2363, 2614, 2665])\n",
      "Batch 72 labels: tensor([   3,   36,   69,   88,  229,  271,  316,  333,  529,  534,  778, 1002,\n",
      "        1086, 1108, 1207, 1528, 1621, 1625, 1771, 1805, 1822, 1877, 1944, 2008,\n",
      "        2054, 2066, 2181, 2248, 2418, 2510, 2567, 2666])\n",
      "Batch 73 labels: tensor([  60,   91,  256,  260,  326,  327,  418,  451,  609,  655,  660,  679,\n",
      "         681,  877,  884, 1127, 1139, 1145, 1189, 1190, 1202, 1260, 1571, 1663,\n",
      "        1945, 1955, 2336, 2499, 2504, 2528, 2605, 2643])\n",
      "Batch 74 labels: tensor([ 125,  199,  337,  511,  602,  718,  889, 1012, 1120, 1344, 1363, 1391,\n",
      "        1454, 1459, 1466, 1538, 1555, 1583, 1671, 2162, 2227, 2242, 2246, 2274,\n",
      "        2359, 2368, 2404, 2512, 2561, 2573, 2613, 2718])\n",
      "Batch 75 labels: tensor([ 117,  168,  416,  463,  571,  657,  706,  738,  742,  779,  797,  800,\n",
      "         835,  855,  893,  902,  981, 1007, 1264, 1309, 1427, 1474, 1598, 1610,\n",
      "        1922, 1962, 2164, 2313, 2369, 2594, 2603, 2681])\n",
      "Batch 76 labels: tensor([  65,  127,  252,  366,  465,  479,  620,  656,  795,  992, 1164, 1432,\n",
      "        1507, 1549, 1560, 1720, 1758, 1783, 1793, 1865, 1879, 1935, 2023, 2068,\n",
      "        2142, 2302, 2400, 2448, 2517, 2531, 2592, 2635])\n",
      "Batch 77 labels: tensor([ 159,  209,  235,  513,  785,  940, 1188, 1192, 1221, 1293, 1333, 1343,\n",
      "        1381, 1617, 1666, 1714, 1724, 2085, 2167, 2476, 2501])\n",
      "Epoch 1/10, Train Loss: 7.9376, Train Accuracy: 0.0024, Val Loss: 8.1410, Val Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "\n",
    "# Define MobileNetV2 + LSTM Model\n",
    "class MobileNetLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(MobileNetLSTM, self).__init__()\n",
    "        \n",
    "        # MobileNetV2 backbone\n",
    "        self.mobilenet = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "        self.mobilenet.eval()  # Set to evaluation mode\n",
    "\n",
    "        # Assuming the output feature size after MobileNet is 1280 (channels)\n",
    "        # Adjusting the LSTM input size\n",
    "        self.input_dim = 1280  # This should be the last dimension of MobileNet output\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer after LSTM\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # If input has 1 channel (grayscale), convert to 3 channels (RGB)\n",
    "        if x.size(1) == 1:\n",
    "            x = x.repeat(1, 3, 1, 1)  # Repeat the single channel to 3 channels\n",
    "    \n",
    "        # Pass through MobileNet\n",
    "        x = self.mobilenet(x)\n",
    "        x = x.mean(dim=2)  # Reducing the height dimension (averaging)\n",
    "    \n",
    "        # Debug: Print shape before passing to LSTM\n",
    "        #print(f\"Shape after MobileNet: {x.shape}\")\n",
    "        \n",
    "        # Flatten the output to match the LSTM input size (batch_size, seq_len, feature_dim)\n",
    "        x = x.view(x.size(0), -1, self.input_dim)  # Flatten to (batch_size, seq_len, 1280)\n",
    "    \n",
    "        # Debug: Print shape after reshaping for LSTM\n",
    "        #print(f\"Shape before LSTM: {x.shape}\")\n",
    "    \n",
    "        # Pass through LSTM\n",
    "        x, _ = self.lstm(x)\n",
    "    \n",
    "        # Pass through fully connected layer\n",
    "        x = self.fc(x[:, -1, :])  # Take the last output of the LSTM sequence\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
    "    \"\"\"\n",
    "    Train MobileNetV2 + LSTM model.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0.0, 0\n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            # Check label values\n",
    "            assert labels.min() >= 0, f\"Labels contain negative values: {labels.min()}\"\n",
    "            assert labels.max() < num_classes, f\"Labels contain values greater than or equal to {num_classes}: {labels.max()}\"\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            # Assert correct shape of outputs and labels\n",
    "            assert outputs.size(0) == labels.size(0), f\"Mismatch between batch size of outputs and labels: {outputs.size(0)} vs {labels.size(0)}\"\n",
    "            assert outputs.size(1) == num_classes, f\"Mismatch between output size and number of classes: {outputs.size(1)} vs {num_classes}\"\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * features.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                outputs = model(features)\n",
    "\n",
    "                # Assert labels are in range\n",
    "                assert labels.min() >= 0 and labels.max() < num_classes, f\"Labels must be between 0 and {num_classes - 1}. Found labels: {labels.min()} - {labels.max()}\"\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * features.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{epochs}, \"\n",
    "            f\"Train Loss: {train_loss / len(train_loader.dataset):.4f}, \"\n",
    "            f\"Train Accuracy: {train_correct / len(train_loader.dataset):.4f}, \"\n",
    "            f\"Val Loss: {val_loss / len(val_loader.dataset):.4f}, \"\n",
    "            f\"Val Accuracy: {val_correct / len(val_loader.dataset):.4f}\"\n",
    "        )\n",
    "    model.summary()\n",
    "    print(\"Training completed.\")\n",
    "    return model\n",
    "    \n",
    "# Main Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Load preprocessed data\n",
    "    train_data = torch.load(\"train_data.pt\")\n",
    "    val_data = torch.load(\"val_data.pt\")\n",
    "    \n",
    "    #print(\"Train Data Keys:\", train_data.keys())\n",
    "    #print(\"Train Data Features Shape:\", train_data[\"features\"][0].shape if \"features\" in train_data else \"Missing\")\n",
    "    #print(\"Train Data Labels:\", train_data[\"labels\"][:5] if \"labels\" in train_data else \"Missing\")\n",
    "    \n",
    "    #print(\"Validation Data Keys:\", val_data.keys())\n",
    "    #print(\"Validation Data Features Shape:\", val_data[\"features\"][0].shape if \"features\" in val_data else \"Missing\")\n",
    "    #print(\"Validation Data Labels:\", val_data[\"labels\"][:5] if \"labels\" in val_data else \"Missing\")\n",
    "\n",
    "        \n",
    "    # Map labels to numerical indices if they are strings\n",
    "    if isinstance(train_data[\"labels\"][0], str):\n",
    "        all_labels = train_data[\"labels\"] + val_data[\"labels\"]\n",
    "        label_mapping = {label: idx for idx, label in enumerate(set(all_labels))}\n",
    "        train_labels = torch.tensor([label_mapping[label] for label in train_data[\"labels\"]])\n",
    "        val_labels = torch.tensor([label_mapping[label] for label in val_data[\"labels\"]])\n",
    "    else:\n",
    "        train_labels = torch.tensor(train_data[\"labels\"])\n",
    "        val_labels = torch.tensor(val_data[\"labels\"])\n",
    "    \n",
    "    train_features = torch.stack(train_data[\"features\"])  # Convert list to tensor\n",
    "    val_features = torch.stack(val_data[\"features\"])  # Convert list to tensor\n",
    "    \n",
    "    # Now you can proceed with training\n",
    "    print(\"Train Features Shape:\", train_features.shape)\n",
    "    print(\"Train Labels Shape:\", train_labels.shape)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(train_features, train_labels)\n",
    "    val_dataset = TensorDataset(val_features, val_labels)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Define device\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    print(f\"Unique labels: {torch.unique(labels)}\")\n",
    "    # Example usage:\n",
    "    input_dim = 1280  # MobileNet output channels\n",
    "    hidden_dim = 128  # LSTM hidden size\n",
    "    # Combine all unique labels from both datasets\n",
    "    all_labels = torch.cat([train_labels, val_labels])\n",
    "    unique_labels = torch.unique(all_labels)\n",
    "    label_mapping = {label.item(): idx for idx, label in enumerate(unique_labels)}\n",
    "    \n",
    "    # Map train and validation labels\n",
    "    train_labels = torch.tensor([label_mapping[label.item()] for label in train_labels])\n",
    "    val_labels = torch.tensor([label_mapping[label.item()] for label in val_labels])\n",
    "    \n",
    "    # Update num_classes based on unique labels\n",
    "    num_classes = len(unique_labels)\n",
    "    print(\"Updated num_classes:\", num_classes)\n",
    "\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "        # Print unique labels in the batch for debugging\n",
    "        print(f\"Batch {batch_idx+1} labels: {torch.unique(labels)}\")\n",
    "        \n",
    "        # Check if labels are in the correct range\n",
    "        assert labels.min() >= 0, f\"Labels contain negative values: {labels.min()}\"\n",
    "        assert labels.max() < num_classes, f\"Labels contain values greater than or equal to {num_classes}: {labels.max()}\"\n",
    "\n",
    "    model = MobileNetLSTM(input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            if torch.isnan(param.grad).any():\n",
    "                print(\"NaN detected in gradients!\")\n",
    "            if torch.isinf(param.grad).any():\n",
    "                print(\"Inf detected in gradients!\")\n",
    "\n",
    "    # Train the model\n",
    "    trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10)\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            if torch.isnan(param.grad).any():\n",
    "                print(\"NaN detected in gradients!\")\n",
    "            if torch.isinf(param.grad).any():\n",
    "                print(\"Inf detected in gradients!\")\n",
    "\n",
    "    print(\"Model device:\", next(model.parameters()).device)\n",
    "    print(\"Input device:\", features.device)\n",
    "    \n",
    "    # Save the trained model\n",
    "    torch.save(trained_model.state_dict(), \"mobilenet_lstm.pth\")\n",
    "    converted_labels = [label_mapping[label] for label in original_labels]\n",
    "\n",
    "    torch.save({\"features\": features, \"labels\": converted_labels}, \"train_data.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dcb4438-6f73-4163-be29-580875906f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: tensor([   0,    1,    2,  ..., 2722, 2723, 2724])\n",
      "Number of unique classes: 2452\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the Few-Shot Learner combining MAML and Prototypical Networks\n",
    "class FewShotLearner(nn.Module):\n",
    "    def __init__(self, base_model, feature_dim):\n",
    "        super(FewShotLearner, self).__init__()\n",
    "        self.base_model = base_model  # MobileNetV2 + LSTM backbone\n",
    "        self.feature_dim = feature_dim\n",
    "        self.protonet_fc = nn.Linear(feature_dim, feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.base_model(x)\n",
    "        # Prototype network uses transformed embeddings\n",
    "        return self.protonet_fc(features)\n",
    "\n",
    "    def get_embeddings(self, x):\n",
    "        # Directly return base model features\n",
    "        return self.base_model(x)\n",
    "\n",
    "# Helper functions for Prototypical Networks\n",
    "def compute_prototypes(support_features, support_labels):\n",
    "    \"\"\"\n",
    "    Compute class prototypes as the mean feature vector for each class in the support set.\n",
    "    \"\"\"\n",
    "    prototypes = defaultdict(list)\n",
    "    for feature, label in zip(support_features, support_labels):\n",
    "        prototypes[label.item()].append(feature)\n",
    "    return {k: torch.stack(v).mean(0) for k, v in prototypes.items()}\n",
    "\n",
    "def prototypical_loss(query_features, query_labels, prototypes):\n",
    "    \"\"\"\n",
    "    Compute the Prototypical Networks loss using distances to prototypes.\n",
    "    \"\"\"\n",
    "    dists = torch.stack([\n",
    "        torch.cdist(query_features, prototype.unsqueeze(0)) for prototype in prototypes.values()\n",
    "    ], dim=1).squeeze(2)\n",
    "    labels = torch.tensor(list(prototypes.keys()), device=query_features.device)\n",
    "    return nn.CrossEntropyLoss()(dists, query_labels)\n",
    "\n",
    "# MAML Training Loop\n",
    "def maml_train(model, task_loader, optimizer, meta_lr=0.001, inner_steps=1, inner_lr=0.01):\n",
    "    \"\"\"\n",
    "    Train using MAML on sampled tasks.\n",
    "    \"\"\"\n",
    "    meta_optimizer = optim.Adam(model.parameters(), lr=meta_lr)\n",
    "    for task in task_loader:\n",
    "        support, query = task\n",
    "        support_features, support_labels = support\n",
    "        query_features, query_labels = query\n",
    "\n",
    "        # Clone model for inner-loop updates\n",
    "        maml_model = FewShotLearner(model.base_model, model.feature_dim)\n",
    "        maml_model.load_state_dict(model.state_dict())\n",
    "        maml_optimizer = optim.SGD(maml_model.parameters(), lr=inner_lr)\n",
    "\n",
    "        # Inner loop updates\n",
    "        maml_model.train()\n",
    "        for _ in range(inner_steps):\n",
    "            maml_optimizer.zero_grad()\n",
    "            support_preds = maml_model(support_features)\n",
    "            loss = nn.CrossEntropyLoss()(support_preds, support_labels)\n",
    "            loss.backward()\n",
    "            maml_optimizer.step()\n",
    "\n",
    "        # Meta-update\n",
    "        maml_model.eval()\n",
    "        query_preds = maml_model(query_features)\n",
    "        meta_loss = nn.CrossEntropyLoss()(query_preds, query_labels)\n",
    "\n",
    "        meta_optimizer.zero_grad()\n",
    "        meta_loss.backward()\n",
    "        meta_optimizer.step()\n",
    "\n",
    "# Prototypical Training Loop\n",
    "def prototypical_train(model, task_loader, optimizer):\n",
    "    \"\"\"\n",
    "    Train using Prototypical Networks on sampled tasks.\n",
    "    \"\"\"\n",
    "    for task in task_loader:\n",
    "        support, query = task\n",
    "        support_features, support_labels = support\n",
    "        query_features, query_labels = query\n",
    "\n",
    "        # Compute prototypes\n",
    "        model.eval()\n",
    "        support_embeddings = model.get_embeddings(support_features)\n",
    "        prototypes = compute_prototypes(support_embeddings, support_labels)\n",
    "\n",
    "        # Prototypical loss\n",
    "        query_embeddings = model.get_embeddings(query_features)\n",
    "        proto_loss = prototypical_loss(query_embeddings, query_labels, prototypes)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        proto_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Combined Training\n",
    "def train_combined(model, task_loader, maml_steps=1, proto_steps=1, epochs=5):\n",
    "    \"\"\"\n",
    "    Train the model using both MAML and Prototypical Networks.\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        for step in range(maml_steps):\n",
    "            maml_train(model, task_loader, optimizer)\n",
    "        for step in range(proto_steps):\n",
    "            prototypical_train(model, task_loader, optimizer)\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "# Task Loader (Simulating Few-Shot Tasks)\n",
    "def create_task_loader(features, labels, num_tasks=100, k_shot=5, q_query=15):\n",
    "    \"\"\"\n",
    "    Generate tasks for meta-learning with support and query splits.\n",
    "    \"\"\"\n",
    "    task_loader = []\n",
    "    unique_labels = list(set(labels.numpy()))\n",
    "    for _ in range(num_tasks):\n",
    "        selected_labels = np.random.choice(unique_labels, size=len(unique_labels), replace=False)\n",
    "        support, query = [], []\n",
    "        for label in selected_labels:\n",
    "            indices = np.where(labels.numpy() == label)[0]\n",
    "            np.random.shuffle(indices)\n",
    "            support.extend(features[indices[:k_shot]])\n",
    "            query.extend(features[indices[k_shot:k_shot + q_query]])\n",
    "        task_loader.append(((torch.stack(support), torch.tensor(selected_labels)), (torch.stack(query), torch.tensor(selected_labels))))\n",
    "    return task_loader\n",
    "\n",
    "# Main Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Load preprocessed single-word dataset\n",
    "    data = torch.load(\"single_word_data.pt\")\n",
    "    features = torch.stack(data[\"features\"])  # Audio features (mel-spectrograms)\n",
    "    labels = torch.tensor(data[\"labels\"])     # Word labels\n",
    "\n",
    "    # Train/Validation Split\n",
    "    train_features, val_features, train_labels, val_labels = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create Task Loaders\n",
    "    task_loader = create_task_loader(train_features, train_labels)\n",
    "\n",
    "    # Load pretrained MobileNetV2+LSTM model\n",
    "    pretrained_model = MobileNetLSTM(input_dim=features.size(-1), hidden_dim=128, num_classes=len(set(labels.numpy())))\n",
    "    pretrained_model.load_state_dict(torch.load(\"mobilenet_lstm.pth\"))\n",
    "\n",
    "    # Few-Shot Learner\n",
    "    feature_dim = 256  # Dimension after embedding\n",
    "    few_shot_model = FewShotLearner(pretrained_model, feature_dim)\n",
    "\n",
    "    # Train the combined model\n",
    "    train_combined(few_shot_model, task_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
